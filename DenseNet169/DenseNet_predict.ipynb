{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random \n",
    "from shutil import copyfile\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "from catalyst.data import Augmentor\n",
    "import torchxrayvision as xrv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "from catalyst.data import Augmentor\n",
    "from skimage.io import imread, imsave\n",
    "import skimage\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# BORDER_CONSTANT = 0\n",
    "# BORDER_REFLECT = 2\n",
    "# crop_size = 224\n",
    "# scale_size = crop_size * 4\n",
    "\n",
    "# transforms = albu.Compose([\n",
    "#   albu.LongestMaxSize(max_size=scale_size),\n",
    "#   albu.PadIfNeeded(scale_size, scale_size, border_mode=BORDER_CONSTANT),\n",
    "#   albu.RandomCrop(crop_size, crop_size),\n",
    "#   albu.OneOf([\n",
    "#     albu.ShiftScaleRotate( \n",
    "#       shift_limit=0.1,\n",
    "#       scale_limit=0.1,\n",
    "#       rotate_limit=15,\n",
    "#       border_mode=BORDER_REFLECT,\n",
    "#       p=0.5\n",
    "#     ),\n",
    "#     albu.Flip(p=0.5),\n",
    "#     albu.RandomRotate90(p=0.5),     \n",
    "#   ]),\n",
    "#   albu.IAAPerspective(scale=(0.02, 0.05), p=0.3),\n",
    "#   albu.JpegCompression(quality_lower=80),\n",
    "#   ToTensor()\n",
    "# ])\n",
    "\n",
    "# transforms_fn = Augmentor(\n",
    "#     dict_key=\"PA\",\n",
    "#     augment_fn=lambda x: transforms(image=x[0][:, :, None])[\"image\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting efficientnet-pytorch\n",
      "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
      "Requirement already satisfied, skipping upgrade: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch) (1.18.1)\n",
      "Building wheels for collected packages: efficientnet-pytorch\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12420 sha256=78d58dbace76726b46df62bf61e5826bce4a2610f0ece1171a732935c71a4fcd\n",
      "  Stored in directory: /tmp/xdg-cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n",
      "Successfully built efficientnet-pytorch\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_transformer = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "val_transformer = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425\n",
      "118\n",
      "202\n"
     ]
    }
   ],
   "source": [
    "batchsize=10\n",
    "def read_txt(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        lines = f.readlines()\n",
    "    txt_data = [line.strip() for line in lines]\n",
    "    return txt_data\n",
    "\n",
    "class CovidCTDataset(Dataset):\n",
    "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt_path (string): Path to the txt file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        File structure:\n",
    "        - root_dir\n",
    "            - CT_COVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "            - CT_NonCOVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.txt_path = [txt_COVID,txt_NonCOVID]\n",
    "        self.classes = ['CT_COVID', 'CT_NonCOVID']\n",
    "        self.num_cls = len(self.classes)\n",
    "        self.img_list = []\n",
    "        for c in range(self.num_cls):\n",
    "            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n",
    "            self.img_list += cls_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.img_list[idx][0]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sample = {'img': image,\n",
    "                  'label': int(self.img_list[idx][1])}\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    trainset = CovidCTDataset(root_dir='new_data/4.4_image',\n",
    "                              txt_COVID='new_data/newtxt/train.txt',\n",
    "                              txt_NonCOVID='old_data/oldtxt/trainCT_NonCOVID.txt',\n",
    "                              transform= train_transformer)\n",
    "    valset = CovidCTDataset(root_dir='new_data/4.4_image',\n",
    "                              txt_COVID='new_data/newtxt/val.txt',\n",
    "                              txt_NonCOVID='old_data/oldtxt/valCT_NonCOVID.txt',\n",
    "                              transform= val_transformer)\n",
    "    testset = CovidCTDataset(root_dir='new_data/4.4_image',\n",
    "                              txt_COVID='new_data/newtxt/test.txt',\n",
    "                              txt_NonCOVID='old_data/oldtxt/testCT_NonCOVID.txt',\n",
    "                              transform= val_transformer)\n",
    "    print(trainset.__len__())\n",
    "    print(valset.__len__())\n",
    "    print(testset.__len__())\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n",
    "    val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)\n",
    "    test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_index, batch_samples in enumerate(train_dataloader):      \n",
    "#         data, target = batch_samples[0], batch_samples[1]\n",
    "# skimage.io.imshow(data[0,1,:,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "alpha = None\n",
    "device = 'cuda'\n",
    "def train(optimizer, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    for batch_index, batch_samples in enumerate(train_loader):\n",
    "        \n",
    "        # move data to device\n",
    "        data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "        data = data[:, 0, :, :]\n",
    "        data = data[:, None, :, :]\n",
    "#         data, targets_a, targets_b, lam = mixup_data(data, target.long(), alpha, use_cuda=True)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        criteria = nn.CrossEntropyLoss()\n",
    "        loss = criteria(output, target.long())\n",
    "#         loss = mixup_criterion(criteria, output, targets_a, targets_b, lam)\n",
    "        train_loss += criteria(output, target.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "    \n",
    "        # Display progress and write to tensorboard\n",
    "        if batch_index % bs == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
    "                epoch, batch_index, len(train_loader),\n",
    "                100.0 * batch_index / len(train_loader), loss.item()/ bs))\n",
    "    \n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
    "        100.0 * train_correct / len(train_loader.dataset)))\n",
    "    f = open('model_result/{}.txt'.format(modelname), 'a+')\n",
    "    f.write('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
    "        100.0 * train_correct / len(train_loader.dataset)))\n",
    "    f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    \n",
    "    \n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    # Don't update model\n",
    "    with torch.no_grad():\n",
    "        tpr_list = []\n",
    "        fpr_list = []\n",
    "        \n",
    "        predlist=[]\n",
    "        scorelist=[]\n",
    "        targetlist=[]\n",
    "        # Predict\n",
    "        for batch_index, batch_samples in enumerate(val_loader):\n",
    "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "            data = data[:, 0, :, :]\n",
    "            data = data[:, None, :, :]\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criteria(output, target.long())\n",
    "            score = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "#             print('target',target.long()[:, 2].view_as(pred))\n",
    "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "            \n",
    "#             print(output[:,1].cpu().numpy())\n",
    "#             print((output[:,1]+output[:,0]).cpu().numpy())\n",
    "#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n",
    "            targetcpu=target.long().cpu().numpy()\n",
    "            predlist=np.append(predlist, pred.cpu().numpy())\n",
    "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
    "            targetlist=np.append(targetlist,targetcpu)\n",
    "           \n",
    "          \n",
    "    return targetlist, scorelist, predlist\n",
    "    \n",
    "    # Write to tensorboard\n",
    "#     writer.add_scalar('Test Accuracy', 100.0 * correct / len(test_loader.dataset), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    \n",
    "    \n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    # Don't update model\n",
    "    with torch.no_grad():\n",
    "        tpr_list = []\n",
    "        fpr_list = []\n",
    "        \n",
    "        predlist=[]\n",
    "        scorelist=[]\n",
    "        targetlist=[]\n",
    "        # Predict\n",
    "        for batch_index, batch_samples in enumerate(test_loader):\n",
    "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "            data = data[:, 0, :, :]\n",
    "            data = data[:, None, :, :]\n",
    "#             print(target)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criteria(output, target.long())\n",
    "            score = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "#             print('target',target.long()[:, 2].view_as(pred))\n",
    "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "#             TP += ((pred == 1) & (target.long()[:, 2].view_as(pred).data == 1)).cpu().sum()\n",
    "#             TN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n",
    "# #             # FN    predict 0 label 1\n",
    "#             FN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 1)).cpu().sum()\n",
    "# #             # FP    predict 1 label 0\n",
    "#             FP += ((pred == 1) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n",
    "#             print(TP,TN,FN,FP)\n",
    "            \n",
    "            \n",
    "#             print(output[:,1].cpu().numpy())\n",
    "#             print((output[:,1]+output[:,0]).cpu().numpy())\n",
    "#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n",
    "            targetcpu=target.long().cpu().numpy()\n",
    "            predlist=np.append(predlist, pred.cpu().numpy())\n",
    "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
    "            targetlist=np.append(targetlist,targetcpu)\n",
    "           \n",
    "    return targetlist, scorelist, predlist\n",
    "    \n",
    "    # Write to tensorboard\n",
    "#     writer.add_scalar('Test Accuracy', 100.0 * correct / len(test_loader.dataset), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %CheXNet pretrain\n",
    "# class DenseNet121(nn.Module):\n",
    "#     \"\"\"Model modified.\n",
    "\n",
    "#     The architecture of our model is the same as standard DenseNet121\n",
    "#     except the classifier layer which has an additional sigmoid function.\n",
    "\n",
    "#     \"\"\"\n",
    "#     def __init__(self, out_size):\n",
    "#         super(DenseNet121, self).__init__()\n",
    "#         self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "#         num_ftrs = self.densenet121.classifier.in_features\n",
    "#         self.densenet121.classifier = nn.Sequential(\n",
    "#             nn.Linear(num_ftrs, out_size),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.densenet121(x)\n",
    "#         return x\n",
    "  \n",
    "\n",
    "# device = 'cuda'\n",
    "# CKPT_PATH = 'model.pth.tar'\n",
    "# N_CLASSES = 14\n",
    "\n",
    "# DenseNet121 = DenseNet121(N_CLASSES).cuda()\n",
    "\n",
    "# CKPT_PATH = './CheXNet/model.pth.tar'\n",
    "\n",
    "# if os.path.isfile(CKPT_PATH):\n",
    "#     checkpoint = torch.load(CKPT_PATH)        \n",
    "#     state_dict = checkpoint['state_dict']\n",
    "#     remove_data_parallel = False\n",
    "\n",
    "\n",
    "#     pattern = re.compile(\n",
    "#         r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
    "#     for key in list(state_dict.keys()):\n",
    "#         match = pattern.match(key)\n",
    "#         new_key = match.group(1) + match.group(2) if match else key\n",
    "#         new_key = new_key[7:] if remove_data_parallel else new_key\n",
    "#         new_key = new_key[7:]\n",
    "#         state_dict[new_key] = state_dict[key]\n",
    "#         del state_dict[key]\n",
    "\n",
    "\n",
    "#     DenseNet121.load_state_dict(checkpoint['state_dict'])\n",
    "#     print(\"=> loaded checkpoint\")\n",
    "# #     print(densenet121)\n",
    "# else:\n",
    "#     print(\"=> no checkpoint found\")\n",
    "\n",
    "# # for parma in DenseNet121.parameters():\n",
    "# #         parma.requires_grad = False\n",
    "# DenseNet121.densenet121.classifier._modules['0'] = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
    "# DenseNet121.densenet121.features.conv0 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "# # print(DenseNet121)\n",
    "# model = DenseNet121.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DenseNet\n",
    "\n",
    "class DenseNetModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Pass in parsed HyperOptArgumentParser to the model\n",
    "        :param hparams:\n",
    "        \"\"\"\n",
    "        super(DenseNetModel, self).__init__()\n",
    "\n",
    "        self.dense_net = xrv.models.DenseNet(num_classes=2)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.dense_net(x)\n",
    "        return logits\n",
    "    \n",
    "model = DenseNetModel().cuda()\n",
    "modelname = 'DenseNet_medical'\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SimpleCNN\n",
    "class SimpleCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__() # b, 3, 32, 32\n",
    "        layer1 = torch.nn.Sequential() \n",
    "        layer1.add_module('conv1', torch.nn.Conv2d(3, 32, 3, 1, padding=1))\n",
    " \n",
    "        #b, 32, 32, 32\n",
    "        layer1.add_module('relu1', torch.nn.ReLU(True)) \n",
    "        layer1.add_module('pool1', torch.nn.MaxPool2d(2, 2)) # b, 32, 16, 16 //池化为16*16\n",
    "        self.layer1 = layer1\n",
    "        layer4 = torch.nn.Sequential()\n",
    "        layer4.add_module('fc1', torch.nn.Linear(401408, 2))       \n",
    "        self.layer4 = layer4\n",
    " \n",
    "    def forward(self, x):\n",
    "        conv1 = self.layer1(x)\n",
    "        fc_input = conv1.view(conv1.size(0), -1)\n",
    "        fc_out = self.layer4(fc_input)\n",
    " \n",
    "model = SimpleCNN().cuda()\n",
    "modelname = 'SimpleCNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ResNet18\n",
    "import torchvision.models as models\n",
    "model = models.resnet18(pretrained=True).cuda()\n",
    "modelname = 'ResNet18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dense121\n",
    "import torchvision.models as models\n",
    "model = models.densenet121(pretrained=True).cuda()\n",
    "modelname = 'Dense121'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dense169\n",
    "import torchvision.models as models\n",
    "model = models.densenet169(pretrained=True).cuda()\n",
    "modelname = 'Dense169'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /tmp/xdg-cache/torch/checkpoints/resnet50-19c8e357.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 80.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "### ResNet50\n",
    "import torchvision.models as models\n",
    "model = models.resnet50(pretrained=True).cuda()\n",
    "modelname = 'ResNet50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VGGNet\n",
    "import torchvision.models as models\n",
    "model = models.vgg16(pretrained=True)\n",
    "model = model.cuda()\n",
    "modelname = 'vgg16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /tmp/xdg-cache/torch/checkpoints/efficientnet-b0-355c32eb.pth\n",
      "100%|██████████| 20.4M/20.4M [00:01<00:00, 15.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "### efficientNet\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=2)\n",
    "model = model.cuda()\n",
    "modelname = 'efficientNet-b0'\n",
    "\n",
    "\n",
    "# model = EfficientNet.from_name('efficientnet-b1').cuda()\n",
    "# modelname = 'efficientNet_random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/43 (0%)]\tTrain Loss: 0.001194\n",
      "Train Epoch: 1 [10/43 (23%)]\tTrain Loss: 0.000765\n",
      "Train Epoch: 1 [20/43 (47%)]\tTrain Loss: 0.000440\n",
      "Train Epoch: 1 [30/43 (70%)]\tTrain Loss: 0.005412\n",
      "Train Epoch: 1 [40/43 (93%)]\tTrain Loss: 0.000066\n",
      "\n",
      "Train set: Average loss: 0.0097, Accuracy: 413/425 (97%)\n",
      "\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.61066816e-07 2.19686350e-04 1.09207009e-04 1.38329324e-06\n",
      " 1.74169600e-07 8.86503528e-07 1.91127256e-04 3.48256544e-05\n",
      " 5.77295395e-07 5.33739746e-01 1.68181886e-03 1.25733379e-04\n",
      " 3.68494079e-06 9.73881483e-01 9.98438418e-01 1.39460377e-09\n",
      " 9.52955403e-08 9.95677173e-01 6.65229885e-03 9.90325809e-01\n",
      " 1.06834896e-01 9.51570511e-01 1.00000000e+00 1.00000000e+00\n",
      " 9.65347230e-01 9.99999881e-01 1.00000000e+00 7.39734678e-04\n",
      " 6.09964479e-09 1.36571532e-06 7.85394572e-04 3.12878261e-03\n",
      " 5.91699500e-03 2.83862409e-08 1.89166894e-06 4.05269568e-10\n",
      " 2.57622537e-06 2.45931044e-01 4.96198097e-03 3.10703646e-02\n",
      " 4.26171094e-01 2.53016653e-04 6.91005811e-02 9.35216667e-06\n",
      " 1.01532166e-07 2.55962732e-05 1.34496472e-03 7.92237639e-01\n",
      " 6.00679994e-01 5.67071497e-01 3.12391400e-01 7.71075079e-08\n",
      " 2.31184390e-06 5.94161565e-06 8.88441689e-03 3.93685950e-05\n",
      " 9.69961107e-01 4.03123204e-06 1.19935554e-02 1.18637216e-08\n",
      " 9.99999881e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 3.71124148e-02 1.29626726e-03 9.55114683e-06 1.00000000e+00\n",
      " 9.70766962e-01 7.97879934e-01 3.29479307e-01 4.52251852e-01\n",
      " 2.65521575e-02 8.92368674e-01 5.34314752e-01 2.42853863e-03\n",
      " 9.99996781e-01 9.99091387e-01 9.94537652e-01 5.19148886e-01\n",
      " 9.99787748e-01 9.80370283e-01 1.00000000e+00 5.08363359e-03\n",
      " 2.44768616e-03 2.15854142e-02 2.15661265e-02 7.28633534e-03\n",
      " 9.65313017e-01 9.99016285e-01 9.99840140e-01 1.39445728e-02\n",
      " 4.37861448e-03 1.00000000e+00 6.48946807e-05 9.83410776e-01\n",
      " 1.97500549e-02 1.33590907e-01 8.06021035e-01 1.09877914e-01\n",
      " 2.33553708e-01 9.20044839e-01 8.09448838e-01 9.99469221e-01\n",
      " 4.14282650e-01 6.64154947e-01 1.00000000e+00 9.92764056e-01\n",
      " 1.96523871e-03 6.08039260e-01 9.92582202e-01 9.86805186e-02\n",
      " 9.88141418e-01 1.00000000e+00 5.24249449e-02 2.84973681e-02\n",
      " 9.86890197e-01 3.64854068e-01]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0.]\n",
      "Train Epoch: 2 [0/43 (0%)]\tTrain Loss: 0.000070\n",
      "Train Epoch: 2 [10/43 (23%)]\tTrain Loss: 0.026755\n",
      "Train Epoch: 2 [20/43 (47%)]\tTrain Loss: 0.000585\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "bs = 10\n",
    "votenum = 10\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "r_list = []\n",
    "p_list = []\n",
    "acc_list = []\n",
    "AUC_list = []\n",
    "# TP = 0\n",
    "# TN = 0\n",
    "# FN = 0\n",
    "# FP = 0\n",
    "vote_pred = np.zeros(valset.__len__())\n",
    "vote_score = np.zeros(valset.__len__())\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "scheduler = StepLR(optimizer, step_size=1)\n",
    "\n",
    "total_epoch = 3000\n",
    "for epoch in range(1, total_epoch+1):\n",
    "    train(optimizer, epoch)\n",
    "    \n",
    "    targetlist, scorelist, predlist = val(epoch)\n",
    "    print('target',targetlist)\n",
    "    print('score',scorelist)\n",
    "    print('predict',predlist)\n",
    "    vote_pred = vote_pred + predlist \n",
    "    vote_score = vote_score + scorelist \n",
    "\n",
    "    if epoch % votenum == 0:\n",
    "        \n",
    "        # major vote\n",
    "        vote_pred[vote_pred <= (votenum/2)] = 0\n",
    "        vote_pred[vote_pred > (votenum/2)] = 1\n",
    "        vote_score = vote_score/votenum\n",
    "        \n",
    "        print('vote_pred', vote_pred)\n",
    "        print('targetlist', targetlist)\n",
    "        TP = ((vote_pred == 1) & (targetlist == 1)).sum()\n",
    "        TN = ((vote_pred == 0) & (targetlist == 0)).sum()\n",
    "        FN = ((vote_pred == 0) & (targetlist == 1)).sum()\n",
    "        FP = ((vote_pred == 1) & (targetlist == 0)).sum()\n",
    "        \n",
    "        \n",
    "        print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
    "        print('TP+FP',TP+FP)\n",
    "        p = TP / (TP + FP)\n",
    "        print('precision',p)\n",
    "        p = TP / (TP + FP)\n",
    "        r = TP / (TP + FN)\n",
    "        print('recall',r)\n",
    "        F1 = 2 * r * p / (r + p)\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        print('F1',F1)\n",
    "        print('acc',acc)\n",
    "        AUC = roc_auc_score(targetlist, vote_score)\n",
    "        print('AUCp', roc_auc_score(targetlist, vote_pred))\n",
    "        print('AUC', AUC)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         if epoch == total_epoch:\n",
    "        torch.save(model.state_dict(), \"model_backup/{}.pt\".format(modelname))  \n",
    "        \n",
    "        vote_pred = np.zeros(valset.__len__())\n",
    "        vote_score = np.zeros(valset.__len__())\n",
    "        print('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "        epoch, r, p, F1, acc, AUC))\n",
    "\n",
    "        f = open('model_result/{}.txt'.format(modelname), 'a+')\n",
    "        f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "        epoch, r, p, F1, acc, AUC))\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP= 82 TN= 81 FN= 23 FP= 16\n",
      "TP+FP 98\n",
      "precision 0.8367346938775511\n",
      "recall 0.780952380952381\n",
      "F1 0.8078817733990147\n",
      "acc 0.806930693069307\n",
      "AUC 0.860284732449681\n",
      "TP= 82 TN= 81 FN= 23 FP= 16\n",
      "TP+FP 98\n",
      "precision 0.8367346938775511\n",
      "recall 0.780952380952381\n",
      "F1 0.8078817733990147\n",
      "acc 0.806930693069307\n",
      "AUC 0.860284732449681\n",
      "TP= 82 TN= 81 FN= 23 FP= 16\n",
      "TP+FP 98\n",
      "precision 0.8367346938775511\n",
      "recall 0.780952380952381\n",
      "F1 0.8078817733990147\n",
      "acc 0.806930693069307\n",
      "AUC 0.860284732449681\n",
      "TP= 82 TN= 81 FN= 23 FP= 16\n",
      "TP+FP 98\n",
      "precision 0.8367346938775511\n",
      "recall 0.780952380952381\n",
      "F1 0.8078817733990147\n",
      "acc 0.806930693069307\n",
      "AUC 0.860284732449681\n",
      "TP= 82 TN= 81 FN= 23 FP= 16\n",
      "TP+FP 98\n",
      "precision 0.8367346938775511\n",
      "recall 0.780952380952381\n",
      "F1 0.8078817733990147\n",
      "acc 0.806930693069307\n",
      "AUC 0.860284732449681\n",
      "TP= 82 TN= 81 FN= 23 FP= 16\n",
      "TP+FP 98\n",
      "precision 0.8367346938775511\n",
      "recall 0.780952380952381\n",
      "F1 0.8078817733990147\n",
      "acc 0.806930693069307\n",
      "AUC 0.860284732449681\n",
      "TP= 82 TN= 81 FN= 23 FP= 16\n",
      "TP+FP 98\n",
      "precision 0.8367346938775511\n",
      "recall 0.780952380952381\n",
      "F1 0.8078817733990147\n",
      "acc 0.806930693069307\n",
      "AUC 0.860284732449681\n",
      "TP= 82 TN= 81 FN= 23 FP= 16\n",
      "TP+FP 98\n",
      "precision 0.8367346938775511\n",
      "recall 0.780952380952381\n",
      "F1 0.8078817733990147\n",
      "acc 0.806930693069307\n",
      "AUC 0.860284732449681\n",
      "TP= 82 TN= 81 FN= 23 FP= 16\n",
      "TP+FP 98\n",
      "precision 0.8367346938775511\n",
      "recall 0.780952380952381\n",
      "F1 0.8078817733990147\n",
      "acc 0.806930693069307\n",
      "AUC 0.860284732449681\n",
      "TP= 82 TN= 81 FN= 23 FP= 16\n",
      "TP+FP 98\n",
      "precision 0.8367346938775511\n",
      "recall 0.780952380952381\n",
      "F1 0.8078817733990147\n",
      "acc 0.806930693069307\n",
      "AUC 0.860284732449681\n",
      "TP= 82 TN= 81 FN= 23 FP= 16\n",
      "TP+FP 98\n",
      "precision 0.8367346938775511\n",
      "recall 0.780952380952381\n",
      "F1 0.8078817733990147\n",
      "acc 0.806930693069307\n",
      "AUC 0.860284732449681\n",
      "vote_pred [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " The epoch is 10, average recall: 0.7810, average precision: 0.8367,average F1: 0.8079, average accuracy: 0.8069, average AUC: 0.8603\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "bs = 10\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "r_list = []\n",
    "p_list = []\n",
    "acc_list = []\n",
    "AUC_list = []\n",
    "# TP = 0\n",
    "# TN = 0\n",
    "# FN = 0\n",
    "# FP = 0\n",
    "vote_pred = np.zeros(testset.__len__())\n",
    "vote_score = np.zeros(testset.__len__())\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "scheduler = StepLR(optimizer, step_size=1)\n",
    "\n",
    "total_epoch = 10\n",
    "for epoch in range(1, total_epoch+1):\n",
    "    \n",
    "    targetlist, scorelist, predlist = test(epoch)\n",
    "#     print('target',targetlist)\n",
    "#     print('score',scorelist)\n",
    "#     print('predict',predlist)\n",
    "    vote_pred = vote_pred + predlist \n",
    "    vote_score = vote_score + scorelist \n",
    "    \n",
    "    TP = ((predlist == 1) & (targetlist == 1)).sum()\n",
    "    TN = ((predlist == 0) & (targetlist == 0)).sum()\n",
    "    FN = ((predlist == 0) & (targetlist == 1)).sum()\n",
    "    FP = ((predlist == 1) & (targetlist == 0)).sum()\n",
    "\n",
    "    print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
    "    print('TP+FP',TP+FP)\n",
    "    p = TP / (TP + FP)\n",
    "    print('precision',p)\n",
    "    p = TP / (TP + FP)\n",
    "    r = TP / (TP + FN)\n",
    "    print('recall',r)\n",
    "    F1 = 2 * r * p / (r + p)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    print('F1',F1)\n",
    "    print('acc',acc)\n",
    "    AUC = roc_auc_score(targetlist, vote_score)\n",
    "    print('AUC', AUC)\n",
    "\n",
    "    if epoch % votenum == 0:\n",
    "        \n",
    "        # major vote\n",
    "        vote_pred[vote_pred <= (votenum/2)] = 0\n",
    "        vote_pred[vote_pred > (votenum/2)] = 1\n",
    "        \n",
    "#         print('vote_pred', vote_pred)\n",
    "#         print('targetlist', targetlist)\n",
    "        TP = ((vote_pred == 1) & (targetlist == 1)).sum()\n",
    "        TN = ((vote_pred == 0) & (targetlist == 0)).sum()\n",
    "        FN = ((vote_pred == 0) & (targetlist == 1)).sum()\n",
    "        FP = ((vote_pred == 1) & (targetlist == 0)).sum()\n",
    "        \n",
    "        print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
    "        print('TP+FP',TP+FP)\n",
    "        p = TP / (TP + FP)\n",
    "        print('precision',p)\n",
    "        p = TP / (TP + FP)\n",
    "        r = TP / (TP + FN)\n",
    "        print('recall',r)\n",
    "        F1 = 2 * r * p / (r + p)\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        print('F1',F1)\n",
    "        print('acc',acc)\n",
    "        AUC = roc_auc_score(targetlist, vote_score)\n",
    "        print('AUC', AUC)\n",
    "        \n",
    "        \n",
    "#         f = open('model_result/{modelname}.txt', 'a+')\n",
    "#         f.write('precision, recall, F1, acc= \\n')\n",
    "#         f.writelines(str(p))\n",
    "#         f.writelines('\\n')\n",
    "#         f.writelines(str(r))\n",
    "#         f.writelines('\\n')\n",
    "#         f.writelines(str(F1))\n",
    "#         f.writelines('\\n')\n",
    "#         f.writelines(str(acc))\n",
    "#         f.writelines('\\n')\n",
    "#         f.close()\n",
    "        \n",
    "        \n",
    "        vote_pred = np.zeros((1,testset.__len__()))\n",
    "        vote_score = np.zeros(testset.__len__())\n",
    "        print('vote_pred',vote_pred)\n",
    "        print('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "        epoch, r, p, F1, acc, AUC))\n",
    "\n",
    "        f = open(f'model_result/test_{modelname}.txt', 'a+')\n",
    "        f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "        epoch, r, p, F1, acc, AUC))\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
